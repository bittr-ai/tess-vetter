{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Extended Vetting Metrics: TOI-5807.01 (TIC 188646744)\n\nThis notebook focuses on the **opt-in extended vetting metrics** (V16–V21) added to `bittr-tess-vetter`.\n\nGoals:\n\n1. Run the baseline `preset=\"default\"` vetting and the `preset=\"extended\"` vetting on the same target\n2. Inspect which extended checks run vs. skip (based on available inputs)\n3. Compare the *additional metrics* produced by V16–V21 without introducing new decision thresholds\n\nThis notebook reuses the tutorial data directory and ephemeris from `04-real-candidate-validation.ipynb`."
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "## Setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "import csv\nfrom pathlib import Path\n\nimport numpy as np\n\n# All imports from the public API\nfrom bittr_tess_vetter.api import (\n    LightCurve,\n    Ephemeris,\n    Candidate,\n    StellarParams,\n    TPFStamp,\n    vet_candidate,\n)\n\nfrom astropy.wcs import WCS"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": "## Load Tutorial Data (Light Curve + TPF)\n\nFor speed and reproducibility, we load the pre-extracted light curve + a representative sector TPF stamp from the tutorial data directory.\n\nIf you want to download fresh data instead, see `04-real-candidate-validation.ipynb`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "TIC_ID = 188646744\nSECTORS = [55, 75, 82, 83]\nDATA_DIR = Path(\"data/tic188646744\")\n\n\ndef load_sector_arrays(sector: int) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n    path = DATA_DIR / f\"sector{sector}_pdcsap.csv\"\n\n    time: list[float] = []\n    flux: list[float] = []\n    flux_err: list[float] = []\n    quality: list[int] = []\n\n    with path.open(newline=\"\") as f:\n        for line in f:\n            if not line.startswith(\"#\"):\n                header = line\n                break\n        else:\n            raise ValueError(f\"Missing CSV header in {path}\")\n\n        reader = csv.DictReader([header] + f.readlines())\n        for row in reader:\n            time.append(float(row[\"time_btjd\"]))\n            flux.append(float(row[\"flux\"]))\n            flux_err.append(float(row[\"flux_err\"]))\n            quality.append(int(row[\"quality\"]))\n\n    t = np.asarray(time)\n    f_arr = np.asarray(flux)\n    e_arr = np.asarray(flux_err)\n    q = np.asarray(quality)\n    ok = q == 0\n    return t[ok], f_arr[ok], e_arr[ok]\n\n\ndef stitch_lightcurves(sectors: list[int]) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n    time_all, flux_all, flux_err_all = [], [], []\n    for sector in sectors:\n        t, f_arr, e_arr = load_sector_arrays(sector)\n        time_all.append(t)\n        flux_all.append(f_arr)\n        flux_err_all.append(e_arr)\n\n    time = np.concatenate(time_all)\n    flux = np.concatenate(flux_all)\n    flux_err = np.concatenate(flux_err_all)\n    sort_idx = np.argsort(time)\n    return time[sort_idx], flux[sort_idx], flux_err[sort_idx]\n\n\ntime, flux, flux_err = stitch_lightcurves(SECTORS)\nlc = LightCurve(time=time, flux=flux, flux_err=flux_err)\n\nprint(f\"Loaded {len(time):,} points from sectors {SECTORS}\")\nprint(f\"Time range: {time.min():.2f} - {time.max():.2f} BTJD\")\nprint(f\"Flux scatter (MAD): {np.median(np.abs(flux - np.median(flux))) * 1e6:.1f} ppm\")\n\n\n# Load one sector TPF stamp (includes an aperture mask)\ntpf_path = DATA_DIR / \"sector83_tpf.npz\"\nif tpf_path.exists():\n    tpf_data = np.load(tpf_path, allow_pickle=True)\n    wcs_header = tpf_data[\"wcs_header\"].item()\n    tpf_wcs = WCS(wcs_header)\n    tpf_stamp = TPFStamp(\n        time=tpf_data[\"time\"],\n        flux=tpf_data[\"flux\"],\n        flux_err=tpf_data[\"flux_err\"],\n        wcs=tpf_wcs,\n        aperture_mask=tpf_data[\"aperture_mask\"],\n        quality=tpf_data[\"quality\"],\n    )\n    print(\n        f\"Loaded TPF stamp: pixels={tpf_stamp.flux.shape[1:]} aperture_pixels={int(tpf_stamp.aperture_mask.sum())}\"\n    )\nelse:\n    tpf_stamp = None\n    print(\"TPF stamp missing; pixel-dependent checks will be skipped.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": "## Candidate Ephemeris + Stellar Params\n\nWe use the same ephemeris parameters as the validation tutorial.\n\nIf you want to re-query ExoFOP or re-fit ephemeris, do that in `04-real-candidate-validation.ipynb` and paste updated values here."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "# Ephemeris (days / BTJD / hours)\nPERIOD_DAYS = 13.0177\nT0_BTJD = 3401.2142\nDURATION_HOURS = 3.42\nDEPTH_PPM = 253.0\n\nephemeris = Ephemeris(\n    period_days=PERIOD_DAYS,\n    t0_btjd=T0_BTJD,\n    duration_hours=DURATION_HOURS,\n)\ncandidate = Candidate(ephemeris=ephemeris, depth_ppm=DEPTH_PPM)\n\n\n# Stellar params used by duration-consistency checks and physical plausibility features\nstellar = StellarParams(\n    teff=6816,\n    radius=1.650,\n    mass=1.47,\n    logg=4.17,\n)\n\nprint(\"Candidate and stellar context configured.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": "## Run Baseline Vetting (`preset=\"default\"`)\n\nThis is the standard 15-check pipeline used by existing tutorials."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "baseline = vet_candidate(\n    lc,\n    candidate,\n    stellar=stellar,\n    tpf=tpf_stamp,\n    network=False,\n    tic_id=TIC_ID,\n    preset=\"default\",\n)\n\nprint(\n    f\"checks={len(baseline.results)} passed={baseline.n_passed} failed={baseline.n_failed} skipped={baseline.n_unknown}\"\n)\nprint([r.id for r in baseline.results])"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": "## Run Extended Vetting (`preset=\"extended\"`)\n\nThis runs the baseline checks plus additional metrics-only diagnostics (V16–V21).\n\nSome extended checks may still return `skipped` depending on which inputs are present (for example, sector-consistency requires host-provided per-sector measurements)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "extended = vet_candidate(\n    lc,\n    candidate,\n    stellar=stellar,\n    tpf=tpf_stamp,\n    network=False,\n    tic_id=TIC_ID,\n    preset=\"extended\",\n)\n\nprint(\n    f\"checks={len(extended.results)} passed={extended.n_passed} failed={extended.n_failed} skipped={extended.n_unknown}\"\n)\nprint([r.id for r in extended.results])"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": "## Compare Baseline vs Extended Results\n\nThis comparison is intentionally metrics-first:\n\n- which checks are new\n- which ones ran vs. skipped\n- what new `details` keys appear\n\nIt does *not* introduce new hard pass/fail thresholds beyond what each check already reports."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "def by_id(bundle):\n    return {r.id: r for r in bundle.results}\n\n\nb0 = by_id(baseline)\nb1 = by_id(extended)\n\nnew_ids = sorted(set(b1.keys()) - set(b0.keys()))\nshared_ids = sorted(set(b1.keys()) & set(b0.keys()))\n\nprint(\"New check IDs in extended:\", new_ids)\n\nprint(\"\\nExtended-only summary:\")\nfor cid in new_ids:\n    r = b1[cid]\n    detail_keys = sorted((r.details or {}).keys())\n    print(\n        f\"- {cid}: status={r.status} confidence={r.confidence} flags={r.flags} detail_keys={detail_keys[:12]}\"\n    )\n\nprint(\"\\nBaseline checks that changed status/confidence between presets:\")\nfor cid in shared_ids:\n    r0, r1 = b0[cid], b1[cid]\n    if (r0.status != r1.status) or (r0.confidence != r1.confidence):\n        print(f\"- {cid}: {r0.status}/{r0.confidence} -> {r1.status}/{r1.confidence}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": "## Deep Dive: Inspect One Extended Check\n\nPick one of V16–V21 that ran and inspect its raw metrics.\n\nIf a check is skipped, check `flags` and `details` for the reason."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "check_id = \"V16\"  # try V17/V18/V19/V20/V21\nr = by_id(extended).get(check_id)\nif r is None:\n    raise ValueError(f\"Missing {check_id} in extended bundle\")\n\nprint(f\"{r.id}: {r.name}\")\nprint(\"status:\", r.status)\nprint(\"confidence:\", r.confidence)\nprint(\"flags:\", r.flags)\nprint(\"details:\")\nfor k, v in (r.details or {}).items():\n    print(f\"  {k}: {v}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": "## Next: Utility Questions to Answer\n\nSuggested, concrete follow-ups for deciding whether V16–V21 are worth running by default (without adding subjective thresholds):\n\n- Do these checks surface *new metrics* that correlate with known false positives or systematics?\n- Across a set of candidates, do extended metrics have stable behavior vs. sector selection / detrending?\n- When a check is skipped, is the reason actionable (missing inputs) or confusing (API friction)?\n\nIf you want, we can extend this notebook to run on a small cohort of TOIs and produce a compact table of V16–V21 metrics for downstream modeling."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
